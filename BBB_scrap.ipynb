{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apscheduler in /usr/local/lib/python3.11/dist-packages (3.11.0)\n",
            "Requirement already satisfied: cloudscraper in /usr/local/lib/python3.11/dist-packages (1.2.71)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.11/dist-packages (from apscheduler) (5.3)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install apscheduler cloudscraper pandas beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx_RlJi-icfk",
        "outputId": "46b73143-aa10-4d30-fece-daf2ff0958fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Scraping page 1...\n",
            "📄 Scraping page 2...\n",
            "📄 Scraping page 3...\n",
            "📄 Scraping page 4...\n",
            "📄 Scraping page 5...\n",
            "📄 Scraping page 6...\n",
            "📄 Scraping page 7...\n",
            "📄 Scraping page 8...\n",
            "📄 Scraping page 9...\n",
            "📄 Scraping page 10...\n",
            "📄 Scraping page 11...\n",
            "📄 Scraping page 12...\n",
            "📄 Scraping page 13...\n",
            "📄 Scraping page 14...\n",
            "📄 Scraping page 15...\n",
            "📄 Scraping page 16...\n",
            "📄 Scraping page 17...\n",
            "📄 Scraping page 18...\n",
            "📄 Scraping page 19...\n",
            "📄 Scraping page 20...\n",
            "                          Name                      Company  \\\n",
            "0            NYC Office Suites            NYC Office Suites   \n",
            "1   Newcastle Investment Corp.   Newcastle Investment Corp.   \n",
            "2  Windsor Development Company  Windsor Development Company   \n",
            "3          Lefrak Organization          Lefrak Organization   \n",
            "4  The Jack Parker Corporation  The Jack Parker Corporation   \n",
            "\n",
            "                         Industry          Contact  \\\n",
            "0  Real Estates, Leasing Services  +1-646-380-2442   \n",
            "1     Trust Company, Real Estates              N/A   \n",
            "2                    Real Estates  +1-212-878-3670   \n",
            "3                    Real Estates  +1-212-708-6600   \n",
            "4                    Real Estates  +1-718-275-2350   \n",
            "\n",
            "                                            Location  \n",
            "0  1350 Avenue of Americas, FL 3, New York, NY 10019  \n",
            "1              1345 Sixth Avenue, New York, NY 10105  \n",
            "2  1345 Avenue of the Americas Suite 200, 2nd Flo...  \n",
            "3        40 W 57th St Fl 23, New York, NY 10019-4001  \n",
            "4             118 W 57th St, New York, NY 10019-3318  \n",
            "✅ Data from 300 contractors saved to bbb_contractors_all_pages.csv\n"
          ]
        }
      ],
      "source": [
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Create a CloudScraper session\n",
        "scraper = cloudscraper.create_scraper()\n",
        "\n",
        "# Base URL (modify `page` parameter)\n",
        "base_url = \"https://www.bbb.org/search?city=new-york&find_country=USA&find_entity=60729-000&find_id=4669_18000&find_latlng=40.762801%2C-73.977818&find_loc=New%20York%2C%20NY&find_text=Real%20Estates&find_type=Category&page=1&sort=Distance&state=NY\"\n",
        "\n",
        "# User-Agent headers\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Pagination settings\n",
        "max_pages = 20  # Adjust this if you need more pages\n",
        "all_data = []\n",
        "\n",
        "for page in range(1, max_pages + 1):\n",
        "    print(f\"📄 Scraping page {page}...\")\n",
        "\n",
        "    # Get page content\n",
        "    url = base_url.format(page)\n",
        "    response = scraper.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"❌ Failed to fetch page {page}. Status Code: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    main_div = soup.find(\"div\", class_=\"stack stack-space-20\")\n",
        "\n",
        "    if not main_div:\n",
        "        print(\"⚠️ No more results found. Stopping.\")\n",
        "        break\n",
        "\n",
        "    # Find all contractor listings\n",
        "    contractors = main_div.find_all(\"div\", class_=\"card result-card\")\n",
        "\n",
        "    if not contractors:\n",
        "        print(\"⚠️ No more contractors found. Ending pagination.\")\n",
        "        break\n",
        "\n",
        "    for contractor in contractors:\n",
        "        try:\n",
        "            name = contractor.find(\"h3\").find(\"a\", class_=\"text-blue-medium\").text.strip()\n",
        "        except AttributeError:\n",
        "            name = \"N/A\"\n",
        "\n",
        "        try:\n",
        "            company = contractor.find(\"span\").text.strip()\n",
        "        except AttributeError:\n",
        "            company = \"N/A\"\n",
        "\n",
        "        try:\n",
        "            industry = contractor.find(\"p\", class_=\"bds-body text-size-4 text-gray-70\").text.strip()\n",
        "        except AttributeError:\n",
        "            industry = \"N/A\"\n",
        "\n",
        "        try:\n",
        "            contact_tag = contractor.find(\"a\", class_=\"text-black\")\n",
        "            contact = contact_tag[\"href\"].replace(\"tel:\", \"\").strip() if contact_tag and \"href\" in contact_tag.attrs else \"N/A\"\n",
        "        except AttributeError:\n",
        "            contact = \"N/A\"\n",
        "\n",
        "        try:\n",
        "            location = contractor.find(\"p\", class_=\"bds-body text-size-5 text-gray-70\").text.strip()\n",
        "        except AttributeError:\n",
        "            location = \"N/A\"\n",
        "\n",
        "        all_data.append({\n",
        "            \"Name\": name,\n",
        "            \"Company\": company,\n",
        "            \"Industry\": industry,\n",
        "            \"Contact\": contact,\n",
        "            \"Location\": location\n",
        "        })\n",
        "\n",
        "    # Avoid getting blocked by the server\n",
        "    time.sleep(2)  # Adjust delay if needed\n",
        "\n",
        "# Save data to DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "print(df.head())  # Display first few rows\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"bbb_contractors_all_pages.csv\", index=False)\n",
        "print(f\"✅ Data from {len(df)} contractors saved to bbb_contractors_all_pages.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
