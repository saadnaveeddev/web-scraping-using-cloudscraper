{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apscheduler in /usr/local/lib/python3.11/dist-packages (3.11.0)\n",
      "Requirement already satisfied: cloudscraper in /usr/local/lib/python3.11/dist-packages (1.2.71)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
      "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.11/dist-packages (from apscheduler) (5.3)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "pip install apscheduler cloudscraper pandas beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://www.realtor.com/realestateagents/columbus_oh?page=1\n",
      "Scraping page 2: https://www.realtor.com/realestateagents/columbus_oh?page=2\n",
      "Scraping page 3: https://www.realtor.com/realestateagents/columbus_oh?page=3\n",
      "Scraping page 4: https://www.realtor.com/realestateagents/columbus_oh?page=4\n",
      "Scraping page 5: https://www.realtor.com/realestateagents/columbus_oh?page=5\n",
      "Scraping page 6: https://www.realtor.com/realestateagents/columbus_oh?page=6\n",
      "Scraping page 7: https://www.realtor.com/realestateagents/columbus_oh?page=7\n",
      "Scraping page 8: https://www.realtor.com/realestateagents/columbus_oh?page=8\n",
      "Scraping page 9: https://www.realtor.com/realestateagents/columbus_oh?page=9\n",
      "Scraping page 10: https://www.realtor.com/realestateagents/columbus_oh?page=10\n",
      "Scraping page 11: https://www.realtor.com/realestateagents/columbus_oh?page=11\n",
      "Scraping page 12: https://www.realtor.com/realestateagents/columbus_oh?page=12\n",
      "Scraping page 13: https://www.realtor.com/realestateagents/columbus_oh?page=13\n",
      "Scraping page 14: https://www.realtor.com/realestateagents/columbus_oh?page=14\n",
      "Scraping page 15: https://www.realtor.com/realestateagents/columbus_oh?page=15\n",
      "Scraping page 16: https://www.realtor.com/realestateagents/columbus_oh?page=16\n",
      "Scraping page 17: https://www.realtor.com/realestateagents/columbus_oh?page=17\n",
      "Scraping page 18: https://www.realtor.com/realestateagents/columbus_oh?page=18\n",
      "Scraping page 19: https://www.realtor.com/realestateagents/columbus_oh?page=19\n",
      "Scraping page 20: https://www.realtor.com/realestateagents/columbus_oh?page=20\n",
      "Scraping page 21: https://www.realtor.com/realestateagents/columbus_oh?page=21\n",
      "Scraping page 22: https://www.realtor.com/realestateagents/columbus_oh?page=22\n",
      "Scraping page 23: https://www.realtor.com/realestateagents/columbus_oh?page=23\n",
      "Scraping page 24: https://www.realtor.com/realestateagents/columbus_oh?page=24\n",
      "Scraping page 25: https://www.realtor.com/realestateagents/columbus_oh?page=25\n",
      "Scraping page 26: https://www.realtor.com/realestateagents/columbus_oh?page=26\n",
      "Scraping page 27: https://www.realtor.com/realestateagents/columbus_oh?page=27\n",
      "Scraping page 28: https://www.realtor.com/realestateagents/columbus_oh?page=28\n",
      "Scraping page 29: https://www.realtor.com/realestateagents/columbus_oh?page=29\n",
      "Scraping page 30: https://www.realtor.com/realestateagents/columbus_oh?page=30\n",
      "Scraping page 31: https://www.realtor.com/realestateagents/columbus_oh?page=31\n",
      "Scraping page 32: https://www.realtor.com/realestateagents/columbus_oh?page=32\n",
      "Scraping page 33: https://www.realtor.com/realestateagents/columbus_oh?page=33\n",
      "Scraping page 34: https://www.realtor.com/realestateagents/columbus_oh?page=34\n",
      "Scraping page 35: https://www.realtor.com/realestateagents/columbus_oh?page=35\n",
      "Scraping page 36: https://www.realtor.com/realestateagents/columbus_oh?page=36\n",
      "Scraping page 37: https://www.realtor.com/realestateagents/columbus_oh?page=37\n",
      "Scraping page 38: https://www.realtor.com/realestateagents/columbus_oh?page=38\n",
      "Scraping page 39: https://www.realtor.com/realestateagents/columbus_oh?page=39\n",
      "Scraping page 40: https://www.realtor.com/realestateagents/columbus_oh?page=40\n",
      "Scraping page 41: https://www.realtor.com/realestateagents/columbus_oh?page=41\n",
      "Scraping page 42: https://www.realtor.com/realestateagents/columbus_oh?page=42\n",
      "Scraping page 43: https://www.realtor.com/realestateagents/columbus_oh?page=43\n",
      "Scraping page 44: https://www.realtor.com/realestateagents/columbus_oh?page=44\n",
      "Scraping page 45: https://www.realtor.com/realestateagents/columbus_oh?page=45\n",
      "Scraping page 46: https://www.realtor.com/realestateagents/columbus_oh?page=46\n",
      "Scraping page 47: https://www.realtor.com/realestateagents/columbus_oh?page=47\n",
      "Scraping page 48: https://www.realtor.com/realestateagents/columbus_oh?page=48\n",
      "Scraping page 49: https://www.realtor.com/realestateagents/columbus_oh?page=49\n",
      "Scraping page 50: https://www.realtor.com/realestateagents/columbus_oh?page=50\n",
      "Scraping page 51: https://www.realtor.com/realestateagents/columbus_oh?page=51\n",
      "Scraping page 52: https://www.realtor.com/realestateagents/columbus_oh?page=52\n",
      "Scraping page 53: https://www.realtor.com/realestateagents/columbus_oh?page=53\n",
      "Scraping page 54: https://www.realtor.com/realestateagents/columbus_oh?page=54\n",
      "Scraping page 55: https://www.realtor.com/realestateagents/columbus_oh?page=55\n",
      "Scraping page 56: https://www.realtor.com/realestateagents/columbus_oh?page=56\n",
      "Scraping page 57: https://www.realtor.com/realestateagents/columbus_oh?page=57\n",
      "Scraping page 58: https://www.realtor.com/realestateagents/columbus_oh?page=58\n",
      "Scraping page 59: https://www.realtor.com/realestateagents/columbus_oh?page=59\n",
      "Scraping page 60: https://www.realtor.com/realestateagents/columbus_oh?page=60\n",
      "Scraping page 61: https://www.realtor.com/realestateagents/columbus_oh?page=61\n",
      "Scraping page 62: https://www.realtor.com/realestateagents/columbus_oh?page=62\n",
      "Scraping page 63: https://www.realtor.com/realestateagents/columbus_oh?page=63\n",
      "Scraping page 64: https://www.realtor.com/realestateagents/columbus_oh?page=64\n",
      "Scraping page 65: https://www.realtor.com/realestateagents/columbus_oh?page=65\n",
      "Scraping page 66: https://www.realtor.com/realestateagents/columbus_oh?page=66\n",
      "Scraping page 67: https://www.realtor.com/realestateagents/columbus_oh?page=67\n",
      "Scraping page 68: https://www.realtor.com/realestateagents/columbus_oh?page=68\n",
      "Scraping page 69: https://www.realtor.com/realestateagents/columbus_oh?page=69\n",
      "Scraping page 70: https://www.realtor.com/realestateagents/columbus_oh?page=70\n",
      "Scraping page 71: https://www.realtor.com/realestateagents/columbus_oh?page=71\n",
      "Scraping page 72: https://www.realtor.com/realestateagents/columbus_oh?page=72\n",
      "Scraping page 73: https://www.realtor.com/realestateagents/columbus_oh?page=73\n",
      "Scraping page 74: https://www.realtor.com/realestateagents/columbus_oh?page=74\n",
      "Scraping page 75: https://www.realtor.com/realestateagents/columbus_oh?page=75\n",
      "Scraping page 76: https://www.realtor.com/realestateagents/columbus_oh?page=76\n",
      "Scraping page 77: https://www.realtor.com/realestateagents/columbus_oh?page=77\n",
      "Scraping page 78: https://www.realtor.com/realestateagents/columbus_oh?page=78\n",
      "Scraping page 79: https://www.realtor.com/realestateagents/columbus_oh?page=79\n",
      "Scraping page 80: https://www.realtor.com/realestateagents/columbus_oh?page=80\n",
      "Scraping page 81: https://www.realtor.com/realestateagents/columbus_oh?page=81\n",
      "Scraping page 82: https://www.realtor.com/realestateagents/columbus_oh?page=82\n",
      "Scraping page 83: https://www.realtor.com/realestateagents/columbus_oh?page=83\n",
      "Scraping page 84: https://www.realtor.com/realestateagents/columbus_oh?page=84\n",
      "Scraping page 85: https://www.realtor.com/realestateagents/columbus_oh?page=85\n",
      "Scraping page 86: https://www.realtor.com/realestateagents/columbus_oh?page=86\n",
      "Scraping page 87: https://www.realtor.com/realestateagents/columbus_oh?page=87\n",
      "Scraping page 88: https://www.realtor.com/realestateagents/columbus_oh?page=88\n",
      "Scraping page 89: https://www.realtor.com/realestateagents/columbus_oh?page=89\n",
      "Scraping page 90: https://www.realtor.com/realestateagents/columbus_oh?page=90\n",
      "Scraping page 91: https://www.realtor.com/realestateagents/columbus_oh?page=91\n",
      "Scraping page 92: https://www.realtor.com/realestateagents/columbus_oh?page=92\n",
      "Scraping page 93: https://www.realtor.com/realestateagents/columbus_oh?page=93\n",
      "Scraping page 94: https://www.realtor.com/realestateagents/columbus_oh?page=94\n",
      "Error: Failed to retrieve page, stopping.\n",
      "Scraping completed! Data saved to real_estate_agents.csv\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL\n",
    "base_url = \"https://www.realtor.com/realestateagents/columbus_oh?page={}\"\n",
    "\n",
    "# Create a CloudScraper session\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "agents_data = []\n",
    "page = 1  # Start from the first page\n",
    "max_pages = 200  # Limit to 200 pages\n",
    "\n",
    "while page <= max_pages:\n",
    "    url = base_url.format(page)\n",
    "    print(f\"Scraping page {page}: {url}\")  # Debugging: Show current page\n",
    "\n",
    "    response = scraper.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error: Failed to retrieve page, stopping.\")\n",
    "        break  # Stop if there's an error (e.g., no more pages)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all agent cards on the page\n",
    "    realtor_cards = soup.find_all(\"div\", class_=\"jsx-3873707352 agent-list-card clearfix\")\n",
    "\n",
    "    if not realtor_cards:  # Stop when no more agent cards are found\n",
    "        print(\"No more agents found, stopping.\")\n",
    "        break\n",
    "\n",
    "    for realtor_card in realtor_cards:\n",
    "        # Extract Name\n",
    "        name_tag = realtor_card.find(\"span\", class_=\"text-bold\")\n",
    "        name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "\n",
    "        # Extract Agency Name\n",
    "        agency_tag = realtor_card.find(\"div\", class_=\"agent-group\")\n",
    "        agency = agency_tag.text.strip() if agency_tag else \"N/A\"\n",
    "\n",
    "        # Extract Experience\n",
    "        experience_tag = realtor_card.find(\"span\", class_=\"bold-text\")\n",
    "        experience = experience_tag.text.strip() if experience_tag else \"N/A\"\n",
    "\n",
    "        # Extract GCI (Activity Range)\n",
    "        gci_container = realtor_card.find(\"div\", class_=\"jsx-3873707352 agent-detail-item\")\n",
    "        gci_tag = gci_container.find(\"span\", class_=\"jsx-3873707352 bold-text\") if gci_container else None\n",
    "        gci = gci_tag.text.strip() if gci_tag else \"N/A\"\n",
    "\n",
    "        # Append agent data to list\n",
    "        agents_data.append({\n",
    "            \"Name\": name,\n",
    "            \"Agency\": agency,\n",
    "            \"Experience\": experience,\n",
    "            \"GCI (Activity Range)\": gci\n",
    "        })\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "    time.sleep(2)  # Delay to prevent blocking\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"real_estate_agents.csv\"\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Name\", \"Agency\", \"Experience\", \"GCI (Activity Range)\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(agents_data)\n",
    "\n",
    "print(f\"Scraping completed! Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **preprocessing**\n",
    "\n",
    "### missing values and duplicate value processing\n",
    "### add rating based on GCI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n",
      "                    Name                          Agency         Experience  \\\n",
      "0     Julian M Mcclurkin                    Real of Ohio  4 years 11 months   \n",
      "1   Phillip Warren Stern                       Realistar                 27   \n",
      "2        Alexander Homer             RE/MAX LEADING EDGE           5 months   \n",
      "3  Christopher Todd Boyd             RE MAX Leading Edge   9 years 5 months   \n",
      "4       CAROLINE SHROYER  Keller Williams Consultants Re  29 years 5 months   \n",
      "\n",
      "  GCI (Activity Range)  \n",
      "0        $240K - $359K  \n",
      "1         $10K - $409K  \n",
      "2       $74.5K - $480K  \n",
      "3       $59.9K - $480K  \n",
      "4        $280K - $400K  \n",
      "Data processing completed! Cleaned data saved to filtered_real_estate_agents.csv\n",
      "\n",
      "Processed Data:\n",
      "                     Name                                         Agency  \\\n",
      "19            Jason Gould                                   Red 1 Realty   \n",
      "18           Amanda Smith  Howard Hanna Real Estate Services - Pataskala   \n",
      "15  MARY MCMICHAEL-LISTON             Keller Williams Classic Properties   \n",
      "7          Carly Kauppila                                   Red 1 Realty   \n",
      "6            Tyler Fickel                                 Ao Real Estate   \n",
      "\n",
      "           Experience       GCI  Rating  \n",
      "19  9 years 11 months  612500.0     4.3  \n",
      "18    6 years 1 month  480000.0     3.6  \n",
      "15  27 years 7 months  442000.0     3.5  \n",
      "7                   8  441500.0     3.9  \n",
      "6                   2  430000.0     4.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV file\n",
    "csv_filename = \"/content/real_estate_agents.csv\"  # Update with actual file\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Display initial data\n",
    "print(\"Initial Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Rename columns properly\n",
    "df.rename(columns={\"GCI (Activity Range)\": \"GCI\"}, inplace=True)\n",
    "\n",
    "# Ensure column names are correct\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert GCI to numerical values\n",
    "def extract_gci(gci):\n",
    "    if isinstance(gci, str):\n",
    "        gci = gci.strip()  # Remove extra spaces\n",
    "        if \"$\" in gci:\n",
    "            try:\n",
    "                amounts = [float(a.replace(\"$\", \"\").replace(\"K\", \"000\").replace(\"M\", \"000000\")) for a in gci.split(\"-\")]\n",
    "                return sum(amounts) / len(amounts)  # Average of min-max range\n",
    "            except:\n",
    "                return np.nan  # Return NaN if conversion fails\n",
    "    return np.nan  # Return NaN if not a valid string\n",
    "\n",
    "df[\"GCI\"] = df[\"GCI\"].apply(extract_gci)\n",
    "\n",
    "# Ensure \"GCI\" column is numeric\n",
    "df[\"GCI\"] = pd.to_numeric(df[\"GCI\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where GCI is still NaN\n",
    "df.dropna(subset=[\"GCI\"], inplace=True)\n",
    "\n",
    "# ✅ Remove duplicate rows BEFORE assigning ratings\n",
    "df = df.drop_duplicates(subset=[\"Name\", \"Agency\", \"Experience\", \"GCI\"], keep=\"first\")\n",
    "\n",
    "# Create a synthetic \"Rating\" column based on GCI\n",
    "def assign_rating(gci):\n",
    "    gci = float(gci)  # Ensure GCI is a number\n",
    "    if gci >= 1000000:\n",
    "        return np.random.uniform(4.5, 5.0)  # Top agents get 4.5 - 5.0 rating\n",
    "    elif gci >= 500000:\n",
    "        return np.random.uniform(4.0, 4.5)\n",
    "    elif gci >= 100000:\n",
    "        return np.random.uniform(3.5, 4.0)\n",
    "    elif gci >= 50000:\n",
    "        return np.random.uniform(3.0, 3.5)\n",
    "    else:\n",
    "        return np.random.uniform(2.0, 3.0)  # Low GCI agents get 2.0 - 3.0\n",
    "\n",
    "df[\"Rating\"] = df[\"GCI\"].apply(assign_rating)\n",
    "\n",
    "# Round ratings to 1 decimal place\n",
    "df[\"Rating\"] = df[\"Rating\"].round(1)\n",
    "\n",
    "# ✅ Remove duplicates again (in case ratings caused minor differences)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Filter out irrelevant leads (Keep only those with GCI > 50K)\n",
    "df = df[df[\"GCI\"] > 50000]  # Fixed threshold\n",
    "\n",
    "# Sort by GCI and Rating\n",
    "df = df.sort_values(by=[\"GCI\", \"Rating\"], ascending=[False, False])\n",
    "\n",
    "# Save cleaned & filtered data to a new CSV file\n",
    "filtered_csv_filename = \"filtered_real_estate_agents.csv\"\n",
    "df.to_csv(filtered_csv_filename, index=False)\n",
    "\n",
    "print(f\"Data processing completed! Cleaned data saved to {filtered_csv_filename}\")\n",
    "\n",
    "# Show final processed data\n",
    "print(\"\\nProcessed Data:\")\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
