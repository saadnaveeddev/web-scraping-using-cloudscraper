{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apscheduler in /usr/local/lib/python3.11/dist-packages (3.11.0)\n",
            "Requirement already satisfied: cloudscraper in /usr/local/lib/python3.11/dist-packages (1.2.71)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.11/dist-packages (from apscheduler) (5.3)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install apscheduler cloudscraper pandas beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tzlocal:/etc/timezone is deprecated on Debian, and no longer reliable. Ignoring.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Scheduler is running... Press Ctrl+C to exit.\n",
            "❌ Scheduler stopped.\n"
          ]
        }
      ],
      "source": [
        "import cloudscraper\n",
        "import time\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "from apscheduler.schedulers.background import BackgroundScheduler\n",
        "\n",
        "# Define the base URL\n",
        "base_url = \"https://www.realtor.com/realestateagents/columbus_oh?page={}\"\n",
        "\n",
        "# Create a CloudScraper session\n",
        "scraper = cloudscraper.create_scraper()\n",
        "\n",
        "def scrape_agents():\n",
        "    agents_data = []\n",
        "    page = 1  # Start from the first page\n",
        "    max_pages = 200  # Limit to 200 pages\n",
        "\n",
        "    while page <= max_pages:\n",
        "        url = base_url.format(page)\n",
        "        print(f\"Scraping page {page}: {url}\")  # Debugging: Show current page\n",
        "\n",
        "        response = scraper.get(url)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Error: Failed to retrieve page, stopping.\")\n",
        "            break  # Stop if there's an error (e.g., no more pages)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find all agent cards on the page\n",
        "        realtor_cards = soup.find_all(\"div\", class_=\"jsx-3873707352 agent-list-card clearfix\")\n",
        "\n",
        "        if not realtor_cards:  # Stop when no more agent cards are found\n",
        "            print(\"No more agents found, stopping.\")\n",
        "            break\n",
        "\n",
        "        for realtor_card in realtor_cards:\n",
        "            # Extract Name\n",
        "            name_tag = realtor_card.find(\"span\", class_=\"text-bold\")\n",
        "            name = name_tag.text.strip() if name_tag else \"N/A\"\n",
        "\n",
        "            # Extract Agency Name\n",
        "            agency_tag = realtor_card.find(\"div\", class_=\"agent-group\")\n",
        "            agency = agency_tag.text.strip() if agency_tag else \"N/A\"\n",
        "\n",
        "            # Extract Experience\n",
        "            experience_tag = realtor_card.find(\"span\", class_=\"bold-text\")\n",
        "            experience = experience_tag.text.strip() if experience_tag else \"N/A\"\n",
        "\n",
        "            # Extract GCI (Activity Range)\n",
        "            gci_container = realtor_card.find(\"div\", class_=\"jsx-3873707352 agent-detail-item\")\n",
        "            gci_tag = gci_container.find(\"span\", class_=\"jsx-3873707352 bold-text\") if gci_container else None\n",
        "            gci = gci_tag.text.strip() if gci_tag else \"N/A\"\n",
        "\n",
        "            # Append agent data to list\n",
        "            agents_data.append({\n",
        "                \"Name\": name,\n",
        "                \"Agency\": agency,\n",
        "                \"Experience\": experience,\n",
        "                \"GCI (Activity Range)\": gci\n",
        "            })\n",
        "\n",
        "        page += 1  # Move to the next page\n",
        "        time.sleep(2)  # Delay to prevent blocking\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_filename = \"real_estate_agents.csv\"\n",
        "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"Name\", \"Agency\", \"Experience\", \"GCI (Activity Range)\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(agents_data)\n",
        "\n",
        "    print(f\"Scraping completed! Data saved to {csv_filename}\")\n",
        "\n",
        "# APScheduler setup\n",
        "scheduler = BackgroundScheduler()\n",
        "scheduler.add_job(scrape_agents, 'interval', hours=6)  # Runs every 6 hours\n",
        "scheduler.start()\n",
        "\n",
        "# Keep script running\n",
        "try:\n",
        "    print(\"⏳ Scheduler is running... Press Ctrl+C to exit.\")\n",
        "    while True:\n",
        "        time.sleep(10)\n",
        "except (KeyboardInterrupt, SystemExit):\n",
        "    scheduler.shutdown()\n",
        "    print(\"❌ Scheduler stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
